<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Dynamic Programming and Reinforcement Learning</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Site Map</div>
<div class="menu-item"><a href="index">Home</a></div>
<div class="menu-item"><a href="outline">Course&nbsp;Outline</a></div>
<div class="menu-item"><a href="course_material">Course&nbsp;Material</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Dynamic Programming and Reinforcement Learning</h1>
</div>
<div class="infoblock">
<div class="blockcontent">
<p>Schedule: Fall 2017, Monday 1:00-4:00pm <br />
Course Number: <br />
Instructor: <a href="http://djrusso.github.io/">Daniel Russo</a> <br />
Contact: djr2174@gsb.columbia.edu <br />
Location: TBD <br />
Office Hours: TBD <br /></p>
</div></div>
<h2>Course Description</h2>
<p>This course offers an advanced introduction Markov Decision Processes (MDPs)&ndash;a formalization of the problem of optimal sequential decision making under
uncertainty&ndash;and Reinforcement Learning (RL)&ndash;a paradigm for learning from data to make near optimal sequential decisions.  The first part of the course 
will cover foundational material on MDPs. We'll then look at the problem of estimating long run value from data, including popular RL algorithms like
temporal difference learning and Q-learning. The final part of the course looks at the design and analysis of efficient exploration algorithms, i.e. those 
that intelligently probe the environment to collect data that improves decision quality. This a doctoral level course. Students should have experience 
with mathematical proofs, coding for numerical computation, and the basics of statistics, optimization, and stochastic processes.</p>
<h2>Course Requirements </h2>
<p>There will be some homework problems in the beginning of class covering fundemental material on MDPs. Afterward, the course will run like
a doctoral seminar. You will be expected to engage with the material and to read some papers outside of class. The main assignment will be a course project,
which could involve literature review, implementation of algorithms, or original research. </p>
<h2>Textbooks</h2>
<p><b>Required:</b> <a href="http://www.athenasc.com/dpbook.html">Dynamic Programming and Optimal Control, Vol II, Dimitris Bertsekas</a> <br />
This textbook will be our main reference on MDPs, and covers a lot of material on approximate DP and reinforcement learning.
It can be purchased for $89 from the publisher. If you plan to work in this area, I reccomend also purchasing volume I. </p>
<p>Reccomended: <a href="http://incompleteideas.net/sutton/book/bookdraft2017june19.pdf">Reinforcement Learning: An Introduction, Second Edition, Richard Sutton and Andrew Barto</a><br />
A pdf of the working draft is freely available. </p>
<p>Reccomended: <a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning, Csaba Czepesv√°ri</a><br />
A consise treatment, also freely available.</p>
<div id="footer">
<div id="footer-text">
Page generated 2017-08-02 14:41:14 China Standard Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
